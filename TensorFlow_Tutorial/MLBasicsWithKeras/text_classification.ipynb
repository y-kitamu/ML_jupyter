{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews (80.23 MiB) to /home/kitamura/tensorflow_datasets/imdb_reviews/subwords8k/0.1.0...\u001b[0m\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/kitamura/tensorflow_datasets/imdb_reviews/subwords8k/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k',\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    as_supervised=True,\n",
    "    with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocaburaly size : 8185\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocaburaly size : {}\".format(encoder.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string is [4025, 222, 6307, 2327, 4043, 2120]\n",
      "The original string : Hello TensorFlow\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"Hello TensorFlow\"\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print(\"Encoded string is {}\".format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print(\"The original string : {}\".format(original_string))\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025 ----> Hell\n",
      "222  ----> o \n",
      "6307 ----> Ten\n",
      "2327 ----> sor\n",
      "4043 ----> Fl\n",
      "2120 ----> ow\n"
     ]
    }
   ],
   "source": [
    "for ts in encoded_string:\n",
    "    print(\"{:<4} ----> {}\".format(ts, encoder.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text :  [ 249    4  277  309  560    6 6639 4574    2   12   31 7759 3525 2128\n",
      "   93 2306   43 2312 2527    6   30 1334 8044   24   10   16   10   17\n",
      "  977   30  815 3339   41  841 7911  376 7974 1923    6  607  219   44\n",
      "  242 1603   11 4329  102 2821 1139    2  969  161   51   18    4 5844\n",
      " 2820  123 4493   40    6 4571   13  117   35  289  850  455   50  460\n",
      " 6359 1069  343   20    1 3733 3511 7670    3  147    4  336    2   42\n",
      "   18    4 3422  409 3533  871 2836  311    5 5080 1209    3  183  117\n",
      "   35 1187    5 1955   11    1  226 7745    3  183 1466 7359 7961 1466\n",
      "  665    2 6854 3178 1377 6266 1447  297    2 5797   36 4740  847 8050\n",
      "    2    5 1929 1631 5986   22 5541 5688    5 1036 3746 8050    3   69\n",
      "  264   35    4 4224    2   26   42   18    4  474 7968    8 1626   24\n",
      "   10   16   10   17  134   15    9    1  435   13    9   55  598 2357\n",
      "   48   30 6611 8044    3  500    1  101    6 2793    2 6548 7961 5861\n",
      "  311    9  139 5398 2270   11 3674 1642   25 7796 7961 5012    3  404\n",
      "    6    1  495 2632    5 5550  353    9  181  741    2   26 7297    6\n",
      "    1 7890 8043    2 6452 8050    2    5 6082  626 6854 3178 1064 8050\n",
      "    3   19 2227    9   56    4 1593    2 5366  194 2808  106    1   98\n",
      " 1451 1377   43    4 7757 1203  316   89    4 1334 1209    3  135 7968\n",
      "    8   33   81 6639 7837   44  208   24   10   16   10   17 7968 6548\n",
      " 7962 1251   25    1  108  778    2    9   97 7269 7961    7    1 1961\n",
      "    3   19 7483 2632    6 7255 5365    9 2125  652 4831  104    2   48\n",
      "    1 2880 1477   34 1992 1577 7961    7    1 6064  360 2387 1209 2356\n",
      "    3   19  724 1882 7974  177  256    1 1992 1577 7961    5 4022 5062\n",
      "  863    2   11   73    1 5758    9 6460  453  130    4 1668    2    4\n",
      " 2403  120  538 7968   21   31   93  193 1257    3 3371  713 6626   49\n",
      "    9   41 5234 7961 3864   24   10   16   10   17  128    1 3246  858\n",
      " 5550  353    9  114  224    2    1 2398  339 2493 3553 2975    6    1\n",
      " 2696    2    1 1183 8029    2 4967 1274    2 1036 3746   47    5 5541\n",
      " 5688   29   51 3339   56   20 3608   20   19 5057 7961   70   31 5324\n",
      " 8029   24   10   16   10   17  134  144   91    9 3498    3 3669 7961\n",
      " 1325 1093   47    9    4 7046 7961   20    1 5224   47 4296  967  633\n",
      " 1074    2 6279 6548 7961 4438   40   57 2408 2811 5848    7 1506  334\n",
      "   24   10   16   10   17 2130   33 6241 6007  124    2 5863 4685 1397\n",
      " 8045   95  546    4 4296 7961   48 7449 7968    8 2236 8045 7974  272\n",
      "   95  379    4  763 3347   47   22 1379 1004 7961   43  376 7974  652\n",
      " 4831   34  138   20    4   97   68 2902  899  102 3595   89   18 6639\n",
      "  413 7968    8  787    3  147    1 1334    2   42   18 2577 7878 6455\n",
      " 1344    2 5782   34   63    6   30  285 2754    7    1 3130  863    8\n",
      "   11 1147    7 3563 7961 1431   28  271   42   18  114 2135    5 6161\n",
      " 8029  158 3056 3950    8   11    1   65    2   42    9 3339   20  195\n",
      "    6    4  336 5803  810 7974 6770 8030    2    4  599    6 7028 2684\n",
      "  352 2040 6491  316   89    4 5147  125    6 1515    2    1 4812  139\n",
      "    4  933 7974 5568 1339 7974 5299  812 1621  166  475    3 4976 1858\n",
      "    2   67  214  763 3347  440   29  949   22 6639 4574 7968    8  787\n",
      "    3  404    6    1  107 2140 1588   29 5760 2073    3  768  302 6054\n",
      "   49    5 1665    9 2322 1139   36    7 4746  182 3733 1463 4662  353\n",
      " 6279   44    1  226  101 7033 2117   23 1515  120 4735 5210 2701  190\n",
      "    3 6071 7961    9  176 2145  360    7 1537    2  894 1383 6708   49\n",
      " 5788 1990   64 2736    8 1808 2153  100    2    5 2408 2811 5848  265\n",
      "    4 6720   64 5764 1868 7974 7689 2842   24   10   16   10   17 2478\n",
      " 1739    9 2097    2  271   53  110 1336  557 1692 1370 2089 3793    3\n",
      " 4312 2407  895 2026 6924   30 3870  124    3 3266   30    5   82 4370\n",
      "  211    9    1   45   11   73    1  386  327 6905  254  130   46   96\n",
      " 3972 1581  135 7968    8 1370 2089 3793 8050    2   26   39 7968    8\n",
      " 6639 7837  192    5 1884   24   10   16   10   17   12  117  186  136\n",
      "    3   12  117 2215  228    1  650 1771 7974 3642 8044    2    1 4129\n",
      " 7961    6    1 2186    5 2234    3  173    9 1382 7974  887  188   48\n",
      "   68 1127 4388 3305    8   89   37  117 3977   49    4 2797  565   24\n",
      "   10   16   10   17   12 3784   21    7  257   15  467 4854    2   26\n",
      " 8002 7968  111  861   68   52 6690  766  837 1076    3  699 3942   15\n",
      "   76   22  155 2400 3045 1064    3  684  314   57   93  193 5418 8029\n",
      " 7975]\n",
      "Label :  1\n"
     ]
    }
   ],
   "source": [
    "for train_example, train_label in train_data.take(1):\n",
    "    print(\"Encoded text : \", train_example.numpy())\n",
    "    print(\"Label : \", train_label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a lifelong fan of Dickens, I have invariably been disappointed by adaptations of his novels.<br /><br />Although his works presented an extremely accurate re-telling of human life at every level in Victorian Britain, throughout them all was a pervasive thread of humour that could be both playful or sarcastic as the narrative dictated. In a way, he was a literary caricaturist and cartoonist. He could be serious and hilarious in the same sentence. He pricked pride, lampooned arrogance, celebrated modesty, and empathised with loneliness and poverty. It may be a clich√©, but he was a people's writer.<br /><br />And it is the comedy that is so often missing from his interpretations. At the time of writing, Oliver Twist is being dramatised in serial form on BBC television. All of the misery and cruelty is their, but non of the humour, irony, and savage lampoonery. The result is just a dark, dismal experience: the story penned by a journalist rather than a novelist. It's not really Dickens at all.<br /><br />'Oliver!', on the other hand, is much closer to the mark. The mockery of officialdom is perfectly interpreted, from the blustering beadle to the drunken magistrate. The classic stand-off between the beadle and Mr Brownlow, in which the law is described as 'a ass, a idiot' couldn't have been better done. Harry Secombe is an ideal choice.<br /><br />But the blinding cruelty is also there, the callous indifference of the state, the cold, hunger, poverty and loneliness are all presented just as surely as The Master would have wished.<br /><br />And then there is crime. Ron Moody is a treasure as the sleazy Jewish fence, whilst Oliver Reid has Bill Sykes to perfection.<br /><br />Perhaps not surprisingly, Lionel Bart - himself a Jew from London's east-end - takes a liberty with Fagin by re-interpreting him as a much more benign fellow than was Dicken's original. In the novel, he was utterly ruthless, sending some of his own boys to the gallows in order to protect himself (though he was also caught and hanged). Whereas in the movie, he is presented as something of a wayward father-figure, a sort of charitable thief rather than a corrupter of children, the latter being a long-standing anti-semitic sentiment. Otherwise, very few liberties are taken with Dickens's original. All of the most memorable elements are included. Just enough menace and violence is retained to ensure narrative fidelity whilst at the same time allowing for children' sensibilities. Nancy is still beaten to death, Bullseye narrowly escapes drowning, and Bill Sykes gets a faithfully graphic come-uppance.<br /><br />Every song is excellent, though they do incline towards schmaltz. Mark Lester mimes his wonderfully. Both his and my favourite scene is the one in which the world comes alive to 'who will buy'. It's schmaltzy, but it's Dickens through and through.<br /><br />I could go on. I could commend the wonderful set-pieces, the contrast of the rich and poor. There is top-quality acting from more British regulars than you could shake a stick at.<br /><br />I ought to give it 10 points, but I'm feeling more like Scrooge today. Soak it up with your Christmas dinner. No original has been better realised.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode(train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3944\n"
     ]
    }
   ],
   "source": [
    "max_shape = 0\n",
    "for t, l in train_data:\n",
    "    max_shape = max(max_shape, t.numpy().shape[0])\n",
    "    \n",
    "print(max_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OptionsDataset' object has no attribute 'output_shapes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-92f754e37fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     .padded_batch(32, train_data.output_shapes))\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m test_batches = (\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OptionsDataset' object has no attribute 'output_shapes'"
     ]
    }
   ],
   "source": [
    "\n",
    "(train_data, test_data), info = tfds.load(\n",
    "    # Use the version pre-encoded with an ~8k vocabulary.\n",
    "    'imdb_reviews/subwords8k', \n",
    "    # Return the train/test datasets as a tuple.\n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
    "    as_supervised=True,\n",
    "    # Also return the `info` structure. \n",
    "    with_info=True)\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = (\n",
    "    train_data\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(32, train_data.output_shapes))\n",
    "\n",
    "test_batches = (\n",
    "    test_data\n",
    "    .padded_batch(32, train_data.output_shapes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
